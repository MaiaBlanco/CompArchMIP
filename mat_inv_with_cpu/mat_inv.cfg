pipelining,0
ready_mode,0
cycle_time,2

# SPAD Mapped 32x32 float arrays: 
partition,block,A,1024,4,16
partition,block,I,1024,4,16
#partition,cyclic,A,4096,4,32
#partition,cyclic,I,4096,4,32
#partition,cyclic,A,16384,4,32
#partition,cyclic,I,16384,4,32

# For CACHE-mapped, 64x64 float array: 
#cache,A,16384,4
#cache,I,16384,4

# For CACHE-mapped, 32x32 float array:
#cache,A,4096,4
#cache,I,4096,4

unrolling,mat_inv,main_loop,1
# norm_cols loops should have idential unroll factors:
unrolling,mat_inv,norm_cols_A,8
unrolling,mat_inv,norm_cols_I,8
# sub_cols loops should also have identical unroll factors:
unrolling,mat_inv,sub_cols_A,8
unrolling,mat_inv,sub_cols_I,8


# NOTES:
# Loop unrolling and pipelining:
# loops can be unrolled, pipelined, or flattened. Ex:
# flatten,function_name,loop_label
# unrolling,function_name,loop_label,factor
# pipeline,function_name,loop_label
# Partitioning:
# instead of partitioning an array (which means it is transferred to-from accel 
# scratchpad (spad) using DMA, it is possible to use directives like the following:
# acp,A,400,4
# ^   ^  ^  ^
# |   |  |  =- size of each data entry (float = 4 bytes)
# |   |  =---- size of array (number of bytes) # TODO: Based on test_acp this may not be needed...!
# |   =------- The array in question
# =----------- use acp or cache here
#
# ACP means application coherence port, which means the data is copied but modified in a 
# memory-coherent fashing
# cache means that the memory is accessed from cache. AFAIK this does not mean a separate
# cache is created but instead that the accelerator uses the core's L1 cache for data
# Partitioning options (for scratchpad) are block, cyclic, and complete (same as Xilinx's 
# documentation.
